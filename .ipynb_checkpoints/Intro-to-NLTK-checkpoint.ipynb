{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Table of content</h1>\n",
    "\n",
    "A. Brief intro to NLP (intro and sources to learn)\n",
    "- What is NLP?\n",
    "- How is this done with programming?\n",
    "- What then is semantic analysis?\n",
    "- How is semantic analysis useful?\n",
    "\n",
    "B. Project: Tweet sentiment analysis with NLTK:\n",
    "- 1. Explain the projects and our approach\n",
    "- 2. Install things and create a workspace\n",
    "- 3. Get the data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> A. Brief intro in NLP</h1>\n",
    "\n",
    "Answers to:\n",
    "- What is NLP?\n",
    "- How is this done with programming?\n",
    "- What then is semantic analysis?\n",
    "- How is semantic analysis useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NLP workflow](https://miro.medium.com/max/1000/1*BiVCmiQtCBIdBNcaOKjurg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources to learn basic NLP:\n",
    "- 1 entry project: https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72\n",
    "\n",
    "- A tutorial series: https://pythonprogramming.net/tokenizing-words-sentences-nltk-tutorial/\n",
    "\n",
    "- Linguistics crash course: (video series) https://www.youtube.com/watch?v=eDop3FDoUzk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>B. Project: Tweet sentiment analysis with NLTK:</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1. Explain the project</h3>  \n",
    "\n",
    "<b>What we'll the able to do at the end of this project:</b><br>Search tweets and get live tweet update, decide if the tweet is either positive or negative based on our sentiment analysis, and put them all in a live graph.\n",
    "\n",
    "<b>Sentiment analysis approach: </b>\n",
    "\n",
    "High level: Break our labeled dataset into words, check if the word appears more in label \"positive\", then we assume that the word must be critical for a positive text. Same for negative.\n",
    "\n",
    "In detail: \n",
    "\n",
    "- Pre-processing: Start with a labeled dataset, which contains 10,000+ texts that are already labeled pos for positive, or neg for negative.\n",
    "\n",
    "- Parsing: Tokenize the dataset into words, part-of-speech tag the type of the words.\n",
    "\n",
    "- Selecting features: Choose 5000 most popular words as our features, mark it in our dataset.\n",
    "\n",
    "- Modelling: Use Naive Bayes classifier that comes with NLTK to train our data. \n",
    "\n",
    "- Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1. Install things and create a workspace</h3>  \n",
    "\n",
    "Install python3 and other basic modules: Jupyter, Matplotlib, NLTK and Scikit-Learn. (https://www.nltk.org/install.html)\n",
    "\n",
    "Create a directory called intro-nlp.\n",
    "File intro.py ....\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Get data</h3>  \n",
    "\n",
    "Link to get data: \n",
    "Inside directory intro-nlp, create a folder called dataset to save the downloaded files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_pos = open(\"dataset/positive.txt\", \"r\").read()\n",
    "short_neg = open(\"dataset/negative.txt\", \"r\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data content: Movie reviews that are already sorted into positive, and negative files.<br>Data format: In each file, each review is separated by a new line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Text Classification</h3>\n",
    "\n",
    "The goals here are to:\n",
    "- Set up the dataset or featuresets for training and testing: collect all reviews, each review needs to be marked with either \"pos\" or \"neg\"\n",
    "- Prep for feature selection: get all of the single words in our dataset, and they need to be Adjectives only. \n",
    "\n",
    "Create empty variables as holders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a list of tuples. Each tuple contains the actual review and the label, for example: (\"This is a nice movie\", \"pos\")\n",
    "documents = []\n",
    "\n",
    "#all single words in our dataset\n",
    "all_words = [] \n",
    "\n",
    "#Filter only Adjectives\n",
    "allowed_word_types = [\"J\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in short_pos.split(\"\\n\"):\n",
    "    documents.append( (p, \"pos\") )\n",
    "    words = nltk.word_tokenize(p) #break into words only\n",
    "    pos = nltk.pos_tag(words) #part of speeach tagging - tagging the type of the word, ex: Adjective, Verb, Noun, ...\n",
    "    for w in pos:\n",
    "        if w[1][0] in allowed_word_types:\n",
    "            all_words.append(w[0].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in short_neg.split(\"\\n\"):\n",
    "    documents.append((n, \"neg\"))\n",
    "    words = nltk.word_tokenize(n)\n",
    "    neg = nltk.pos_tag(words)\n",
    "    for w in neg:\n",
    "        if w[1][0] in allowed_word_types:\n",
    "            all_words.append(w[0].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10664"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26287"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . '"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find out the most common 15 words\n",
    "# all_words.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the amount of time a specific word appears\n",
    "# all_words[\"great\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h2>Creating Features</h2>\n",
    " \n",
    " - Explain: What is Feature\n",
    " \n",
    " \n",
    " - Select features for this project: Top 5000 most popular adjectives. With these top 5000 words, we try to find out how many times these words appear in negative or positive reviews. If it appears more significantly in a negative review, that means that word is important in a negative reviews. \n",
    " \n",
    " For example, \"wonderful\" is a among the top 5000 popular words. Our training spotted the word \"bad\" in 10 negative reviews, and only 1 negative review. Then if we come accross a piece of texts: \"the storyline is wonderful!\". Then it's very likely that it's a negative comment.\n",
    " \n",
    " \n",
    " - Create featuresets for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get Top 5000 most popular adjectives\n",
    "\n",
    "#calculate the number of time a word is repeated, and put the in a dictionary.\n",
    "all_words = nltk.FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('good', 369),\n",
       " ('more', 331),\n",
       " ('little', 265),\n",
       " ('funny', 245),\n",
       " ('much', 234),\n",
       " ('bad', 234),\n",
       " ('best', 208),\n",
       " ('new', 206),\n",
       " ('own', 185),\n",
       " ('many', 183),\n",
       " ('most', 167),\n",
       " ('other', 167),\n",
       " ('great', 160),\n",
       " ('big', 156),\n",
       " ('few', 139)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find out the most common 15 words\n",
    "all_words.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the amount of time a specific word appears\n",
    "all_words[\"ridiculous\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list to put those features in\n",
    "word_features = []\n",
    "for i in all_words.most_common(5000):\n",
    "    word_features.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good',\n",
       " 'more',\n",
       " 'little',\n",
       " 'funny',\n",
       " 'much',\n",
       " 'bad',\n",
       " 'best',\n",
       " 'new',\n",
       " 'own',\n",
       " 'many']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_features[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find those features and mark them within the document we are using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#send document in, and get returned a dictionary that confirms if the each word in the document is a feature word or not\n",
    "def find_features(document):\n",
    "    words = nltk.word_tokenize(document)\n",
    "    features = {} \n",
    "    for word in word_features:\n",
    "        features[word] = (word in words) #True/False \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(find_features(rev), category) for (rev,category) in documents]\n",
    "#a list of tuples. Each tuple has 2 elements: 1 is a dictionary returned from find_features, 2 is the label: \"pos\"/\"neg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(featuresets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Train the dataset using Naive Bayes Classifier</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain Naive Bayes Classier\n",
    "\n",
    "More details: http://web.stanford.edu/~jurafsky/slp3/4.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rule of thumb 80/20\n",
    "training_set = featuresets[:8530]\n",
    "testing_set = featuresets[8530:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_set is picked for the machine to do the testing. Here, the machine knows if a review is negative or positive.\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing_set is picked for the machine to do the testing. Here, the machine doesn't actually know the result.\n",
    "#What the accuracy method actually does here is give the machine a data set withou any result, let the machine guess itself, \n",
    "#and then test against it, and finally return the result.\n",
    "\n",
    "accuracy = nltk.classify.accuracy(classifier, testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72.39925023430177"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_percentage = accuracy*100\n",
    "accuracy_percentage\n",
    "\n",
    "#the accuracy can change slightly each time we run the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is called supervised machine learning, because we're showing the machine data, and telling it \"hey, this data is positive,\" or \"this data is negative.\" Then, after that training is done, we show the machine some new data and ask the computer, based on what we taught the computer before, what the computer thinks the category of the new data is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "              engrossing = True              pos : neg    =     14.9 : 1.0\n",
      "                mediocre = True              neg : pos    =     14.4 : 1.0\n",
      "                  boring = True              neg : pos    =     14.3 : 1.0\n",
      "                 routine = True              neg : pos    =     13.7 : 1.0\n",
      "                    thin = True              neg : pos    =     13.7 : 1.0\n",
      "                 generic = True              neg : pos    =     12.4 : 1.0\n",
      "                    loud = True              neg : pos    =     12.4 : 1.0\n",
      "                 winning = True              pos : neg    =     11.6 : 1.0\n",
      "                    warm = True              pos : neg    =     11.4 : 1.0\n",
      "                    dull = True              neg : pos    =     11.0 : 1.0\n",
      "                    flat = True              neg : pos    =     10.6 : 1.0\n",
      "                   vivid = True              pos : neg    =     10.3 : 1.0\n",
      "              thoughtful = True              pos : neg    =     10.2 : 1.0\n",
      "               wonderful = True              pos : neg    =     10.2 : 1.0\n",
      "             superficial = True              neg : pos    =      9.7 : 1.0\n",
      "               realistic = True              pos : neg    =      9.6 : 1.0\n",
      "                haunting = True              pos : neg    =      9.6 : 1.0\n",
      "                    lame = True              neg : pos    =      9.0 : 1.0\n",
      "              repetitive = True              neg : pos    =      9.0 : 1.0\n",
      "                annoying = True              neg : pos    =      9.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "#check the most informative words in deciding whether a review is either positive or negative\n",
    "classifier.show_most_informative_features(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Use the sentiment</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(text):\n",
    "    feats = find_features(text)\n",
    "    return classifier.classify(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment(\"It was awesome!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment(\"I hate it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment(\"AMZN do you like it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Evaluation</h3>\n",
    "\n",
    "- the method relies on if specific words appear more in neg/pos reviews and predicts based on words\n",
    "- neutral sentences tend to be classified as negative\n",
    "- Zak please add more :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Get live tweets from Twitter API</h3>\n",
    "\n",
    "Able to connect to 1% of all tweets, that is 1% 500 million tweets per day - still actually a lot of tweets. \n",
    "\n",
    "Explain how to create an API application\n",
    "- a\n",
    "- b\n",
    "- c\n",
    "\n",
    "What we are going to do: \n",
    "- Connect to Twitter API to get access to live tweets\n",
    "- Type in a keyword, gets all of the tweets that contains that keyword\n",
    "- Save all of those tweets in a .txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import Stream, OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "import json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my API access token\n",
    "#I HAVE TO REMEMBER TO HIDE MY TOKENS LATER\n",
    "\n",
    "ckey=\"ASfenRJcyi4b8r9NjPVzFex8m\"\n",
    "csecret=\"WMYSrpfA82DQHNpiTPB6XFkjZgMxFphG1hu4wXKLIoZCZm3aW1\"\n",
    "atoken=\"798767325772029952-lcpNI5IOTozzniYRXblWHcfNYYUJoTa\"\n",
    "asecret=\"fqWAC8kGtzU1CBrxAcwtdd68VADpwZFf5Y9ivsi00qmkb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class listener(StreamListener):\n",
    "\n",
    "    def on_data(self, data):\n",
    "\n",
    "        #load tweets in real time\n",
    "        all_data = json.loads(data)\n",
    "        \n",
    "        #take only text format, and send it through our sentiment\n",
    "        tweet = all_data[\"text\"]\n",
    "        sentiment_value = sentiment(tweet)\n",
    "        print(tweet)\n",
    "        print(sentiment_value)\n",
    "        \n",
    "        #save all in a txt.file to use later for our live graph\n",
    "        output = open(\"out.txt\",\"a\")\n",
    "        output.write(sentiment_value)\n",
    "        output.write('\\n')\n",
    "        output.close()\n",
    "\n",
    "        return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the stream, and enter key word\n",
    "auth = OAuthHandler(ckey, csecret)\n",
    "auth.set_access_token(atoken, asecret)\n",
    "\n",
    "twitterStream = Stream(auth, listener())\n",
    "twitterStream.filter(track=[\"happy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Graphing Live Twitter Sentiment Analysis</h2>\n",
    "\n",
    "Using Matplotlib, a very popular tool for visualization, but won't explain much how it works here. \n",
    "\n",
    "The idea: we saved the sentiment result: pos or neg in the out.txt file, each result is seperated by a new line.\n",
    "So here, we graph the trend:\n",
    "- if positive, the line will go up 1. \n",
    "- if negative, the line will go down 1.\n",
    "\n",
    "Since both Twitter streams, and this Graph run live, we gotta create a new file and run this seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib import style\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "style.use(\"ggplot\")\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "\n",
    "def animate(i):\n",
    "    pullData = open(\"out.txt\",\"r\").read()\n",
    "    lines = pullData.split('\\n')\n",
    "\n",
    "    xar = []\n",
    "    yar = []\n",
    "\n",
    "    x = 0\n",
    "    y = 0\n",
    "\n",
    "    for l in lines[-200:]:\n",
    "        x += 1\n",
    "        if \"pos\" in l:\n",
    "            y += 1\n",
    "        elif \"neg\" in l:\n",
    "            y -= 1\n",
    "\n",
    "        xar.append(x)\n",
    "        yar.append(y)\n",
    "        \n",
    "    ax1.clear()\n",
    "    ax1.plot(xar,yar)\n",
    "    \n",
    "ani = animation.FuncAnimation(fig, animate, interval=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate(i):\n",
    "    pullData = open(\"out.txt\",\"r\").read()\n",
    "    lines = pullData.split('\\n')\n",
    "\n",
    "    xar = []\n",
    "    yar = []\n",
    "\n",
    "    x = 0\n",
    "    y = 0\n",
    "\n",
    "    for l in lines[-200:]:\n",
    "        x += 1\n",
    "        if \"pos\" in l:\n",
    "            y += 1\n",
    "        elif \"neg\" in l:\n",
    "            y -= 1\n",
    "\n",
    "        xar.append(x)\n",
    "        yar.append(y)\n",
    "        \n",
    "    ax1.clear()\n",
    "    ax1.plot(xar,yar)\n",
    "    \n",
    "ani = animation.FuncAnimation(fig, animate, interval=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h3>Saving Classifiers using Pickle</h3>\n",
    "\n",
    "Take the classifier just made and save it in a pickle file, so that you can just use it anywhere else without writing out again all lines of codes above\n",
    "import pickle\n",
    "\n",
    "# save_classifier = open(\"naivebayes.pickle\",\"wb\") #create and open a pickle file, allow writing access\n",
    "# pickle.dump(classifier, save_classifier) #put the classifier in the pickle file\n",
    "# save_classifier.close() #close the file and done!\n",
    "\n",
    "#Now if you want to use it else where for another project, you just need to type in the lines below:\n",
    "# classifier_f = open(\"naivebayes.pickle\", \"rb\") #open the pickle file, allow reading access\n",
    "# classifier = pickle.load(classifier_f) #take the classifier out of the pickle file\n",
    "# classifier_f.close() #close the file and done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
